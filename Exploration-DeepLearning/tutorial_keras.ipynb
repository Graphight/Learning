{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is following a tutorial from [machine learning mastery](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/) just to get familiar with things\n",
    "\n",
    "I am going to dump a lot of the tips I found useful into this notebook.\n",
    "There will also be a lot of links to the authors other works."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "# Breakdown of the data\n",
    "\n",
    "# Input Variables (X):\n",
    "# 1. Number of times pregnant\n",
    "# 2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "# 3. Diastolic blood pressure (mm Hg)\n",
    "# 4. Triceps skin fold thickness (mm)\n",
    "# 5. 2-Hour serum insulin (mu U/ml)\n",
    "# 6. Body mass index (weight in kg/(height in m)^2)\n",
    "# 7. Diabetes pedigree function\n",
    "# 8. Age (years)\n",
    "\n",
    "# Output Variables (y):\n",
    "# 1. Class variable (0 or 1)\n",
    "\n",
    "# The turoial wants us to use the numpy loadtxt\n",
    "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "y = dataset[:, 8]\n",
    "\n",
    "# Here is the equivalent in pandas - just to prove a point\n",
    "# import pandas as pd\n",
    "# pandas_dataset = pd.read_csv(\"pima-indians-diabetes.csv\", delimiter=',', header=None)\n",
    "# pandas_X = pandas_dataset.loc[:, 0:7]\n",
    "# pandas_y = pandas_dataset.loc[:, 8]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the Keras Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing to get right is to ensure the input layer has the right number of input features.\n",
    "This can be specified when creating the first layer with the input_dim argument and setting it to 8 for the 8 input variables.\n",
    "The tutorial asks how to determine the number of nodes in each layer, then links to [another of their posts](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)\n",
    "\n",
    "It used to be the case that Sigmoid and Tanh activation functions were preferred for all layers.\n",
    "These days, better performance is achieved using the ReLU activation function.\n",
    "We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Note***,\n",
    "the most confusing thing here is that the shape of the input to the model is defined as an argument on the first hidden layer.\n",
    "This means that the line of code that adds the first Dense layer is doing 2 things, defining the input or visible layer and the first hidden layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compile the Keras Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When compiling, we must specify some additional properties required when training the network.\n",
    "Remember training a network means finding the best set of weights to map inputs to outputs in our dataset.\n",
    "\n",
    "We must specify the **loss function** to use to evaluate a set of weights, the optimizer is used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n",
    "\n",
    "[How to Choose Loss Functions When Training Deep Learning Neural Networks](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "\n",
    "We will define the **optimizer** as the efficient stochastic gradient descent algorithm “adam“.\n",
    "This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems.\n",
    "[Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "Finally, because it is a classification problem, we will collect and report the classification accuracy, defined via the **metrics** argument"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fit the Keras Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training occurs over epochs and each epoch is split into batches.\n",
    "\n",
    "***Epoch***: One pass through all of the rows in the training dataset.\n",
    "***Batch***: One or more samples considered by the model within an epoch before weights are updated.\n",
    "\n",
    "One epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs.\n",
    "\n",
    "These configurations can be chosen experimentally by trial and error.\n",
    "We want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification.\n",
    "The model will always have some error, but the amount of error will level out after some point for a given model configuration.\n",
    "\n",
    "*This is called **model convergence**.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 0s 647us/step - loss: 3.3875 - accuracy: 0.6458\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 653us/step - loss: 1.9811 - accuracy: 0.6419\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 703us/step - loss: 1.6288 - accuracy: 0.6302\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 726us/step - loss: 1.4135 - accuracy: 0.6367\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 722us/step - loss: 1.2326 - accuracy: 0.6315\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 749us/step - loss: 1.0527 - accuracy: 0.6276\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 704us/step - loss: 0.9253 - accuracy: 0.6406\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 0s 755us/step - loss: 0.8805 - accuracy: 0.6198\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 768us/step - loss: 0.7358 - accuracy: 0.6406\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 736us/step - loss: 0.7312 - accuracy: 0.6497\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 907us/step - loss: 0.6703 - accuracy: 0.6888\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 727us/step - loss: 0.7135 - accuracy: 0.6354\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 676us/step - loss: 0.6517 - accuracy: 0.6745\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.6531 - accuracy: 0.6471\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 750us/step - loss: 0.6378 - accuracy: 0.6654\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 692us/step - loss: 0.6206 - accuracy: 0.6732\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 747us/step - loss: 0.6494 - accuracy: 0.6849\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 733us/step - loss: 0.6372 - accuracy: 0.6654\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 728us/step - loss: 0.6394 - accuracy: 0.6706\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 706us/step - loss: 0.6989 - accuracy: 0.6432\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 762us/step - loss: 0.6165 - accuracy: 0.6810\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 741us/step - loss: 0.6463 - accuracy: 0.6641\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 716us/step - loss: 0.6251 - accuracy: 0.6745\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 708us/step - loss: 0.5878 - accuracy: 0.7044\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 688us/step - loss: 0.6082 - accuracy: 0.6823\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 695us/step - loss: 0.6467 - accuracy: 0.6562\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 748us/step - loss: 0.6140 - accuracy: 0.6940\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 750us/step - loss: 0.6169 - accuracy: 0.6875\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 838us/step - loss: 0.6001 - accuracy: 0.6940\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 750us/step - loss: 0.6131 - accuracy: 0.6914\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 726us/step - loss: 0.5946 - accuracy: 0.6914\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 806us/step - loss: 0.6153 - accuracy: 0.6836\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 726us/step - loss: 0.5915 - accuracy: 0.6979\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.6083 - accuracy: 0.7031\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 764us/step - loss: 0.5888 - accuracy: 0.6979\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 700us/step - loss: 0.6038 - accuracy: 0.6914\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 651us/step - loss: 0.5775 - accuracy: 0.7109\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 655us/step - loss: 0.5712 - accuracy: 0.7227\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 654us/step - loss: 0.5712 - accuracy: 0.7214\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 618us/step - loss: 0.5932 - accuracy: 0.6849\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 636us/step - loss: 0.5771 - accuracy: 0.7174\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 642us/step - loss: 0.5987 - accuracy: 0.6927\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 648us/step - loss: 0.5740 - accuracy: 0.7018\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 613us/step - loss: 0.5945 - accuracy: 0.7018\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 684us/step - loss: 0.5468 - accuracy: 0.7214\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 724us/step - loss: 0.5848 - accuracy: 0.7031\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 704us/step - loss: 0.5695 - accuracy: 0.7188\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 683us/step - loss: 0.5578 - accuracy: 0.7409\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 688us/step - loss: 0.5530 - accuracy: 0.7396\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 625us/step - loss: 0.5806 - accuracy: 0.6953\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 658us/step - loss: 0.5722 - accuracy: 0.7174\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 692us/step - loss: 0.5982 - accuracy: 0.7044\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 625us/step - loss: 0.5689 - accuracy: 0.7044\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 621us/step - loss: 0.5546 - accuracy: 0.7279\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 636us/step - loss: 0.5331 - accuracy: 0.7526\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 694us/step - loss: 0.5857 - accuracy: 0.6784\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 709us/step - loss: 0.5832 - accuracy: 0.7161\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 672us/step - loss: 0.5570 - accuracy: 0.7331\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 650us/step - loss: 0.5596 - accuracy: 0.7148\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 668us/step - loss: 0.5371 - accuracy: 0.7344\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 679us/step - loss: 0.5332 - accuracy: 0.7227\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5483 - accuracy: 0.7214\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 669us/step - loss: 0.5416 - accuracy: 0.7305\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5501 - accuracy: 0.7435\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 671us/step - loss: 0.5335 - accuracy: 0.7383\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 690us/step - loss: 0.5424 - accuracy: 0.7422\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 689us/step - loss: 0.5170 - accuracy: 0.7500\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.5687 - accuracy: 0.7292\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 619us/step - loss: 0.5460 - accuracy: 0.7174\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.5718 - accuracy: 0.7174\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.5837 - accuracy: 0.7083\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 698us/step - loss: 0.5794 - accuracy: 0.7057\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 690us/step - loss: 0.5597 - accuracy: 0.7357\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5632 - accuracy: 0.7292\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 691us/step - loss: 0.5167 - accuracy: 0.7435\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 700us/step - loss: 0.5329 - accuracy: 0.7383\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 688us/step - loss: 0.5211 - accuracy: 0.7539\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 657us/step - loss: 0.5399 - accuracy: 0.7383\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.5142 - accuracy: 0.7539\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 677us/step - loss: 0.5169 - accuracy: 0.7565\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 691us/step - loss: 0.5931 - accuracy: 0.7070\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 652us/step - loss: 0.5635 - accuracy: 0.7161\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 0s 634us/step - loss: 0.5243 - accuracy: 0.7357\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.5117 - accuracy: 0.7435\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 657us/step - loss: 0.5373 - accuracy: 0.7370\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 640us/step - loss: 0.5089 - accuracy: 0.7500\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 625us/step - loss: 0.5213 - accuracy: 0.7422\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 668us/step - loss: 0.5361 - accuracy: 0.7370\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 674us/step - loss: 0.5151 - accuracy: 0.7591\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 615us/step - loss: 0.5110 - accuracy: 0.7461\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 658us/step - loss: 0.5359 - accuracy: 0.7409\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 657us/step - loss: 0.5080 - accuracy: 0.7669\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 674us/step - loss: 0.5460 - accuracy: 0.7370\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 632us/step - loss: 0.6116 - accuracy: 0.7148\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 669us/step - loss: 0.5351 - accuracy: 0.7344\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 638us/step - loss: 0.5153 - accuracy: 0.7370\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 654us/step - loss: 0.4997 - accuracy: 0.7552\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 673us/step - loss: 0.5299 - accuracy: 0.7513\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 684us/step - loss: 0.5400 - accuracy: 0.7383\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 641us/step - loss: 0.5446 - accuracy: 0.7357\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 670us/step - loss: 0.5301 - accuracy: 0.7409\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 669us/step - loss: 0.5584 - accuracy: 0.7344\n",
      "Epoch 103/150\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.5229 - accuracy: 0.7417"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**:\n",
    "Neural nets require ALL inputs to be numeric, make sure that you do some form of encoding on your categoricals before throwing your dataset at the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the Keras Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the tutorial was super lazy here and just evaluated the data on the same dataset it trained on.\n",
    "**DO NOT DO THIS IN THE REAL WORLD**\n",
    "To their credit they did say in a real example you should split into train and test sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate(X, y)\n",
    "print(f\"Accuracy: {accuracy * 100.0:.2f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.predict(X)\n",
    "\n",
    "# If instead you want your predictions as labels\n",
    "threshold = 0.5\n",
    "predictions_crisp = (model.predict(X) > threshold).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_crisp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"The single model gives us an accuracy of: {100.0 * np.mean(predictions_crisp == y.astype(int)):.2f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# My improvements / experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to the stochastic nature of backpropagation we can train the model multiple times and it will be slightly different.\n",
    "This can be advantageous as you can do similar things to ensemble machine learning and stack the outcomes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "# Generate multiple models\n",
    "for i in range(0, 5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation=\"relu\"))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(X, y, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "    _, accuracy = model.evaluate(X, y)\n",
    "    print(f\"Model {i} has accuracy: {accuracy * 100.0:.2f} %\")\n",
    "    key = f\"model_{i}\"\n",
    "    models[key] = model\n",
    "    predictions[key] = (model.predict(X) > threshold).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "group_prediction = []\n",
    "for i in range(0, len(X)):\n",
    "    group_prediction.append(mode([predictions[key][i][0] for key in predictions.keys()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Group think gives us an accuracy of: {100.0 * np.mean(group_prediction == y.astype(int)):.2f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By taking the **mode prediction** of a bunch of models for each data point we can hopefully make a model that is better than average\n",
    "As you can see, if we had just trained a single model we might have been unlucky and got one of the under performing models\n",
    "\n",
    "In essence this is what a lot of fancy research neural nets are doing.\n",
    "By making a swarm of slightly different models that can cover each others weaknesses we end up with a prevailing group model that makes on average better choices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}