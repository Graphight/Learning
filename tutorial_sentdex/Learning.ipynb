{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2538d",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just me following a tutorial from Sentdex\n",
    "# https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL&t=0s\n",
    "\n",
    "# I wanted to familiarise myself with NLTK so it will probably be pretty messy and useless.\n",
    "# Keeping it around so I can refer back to it if I need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a952c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b97a24",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c34ff00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3932b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will need to be run to setup the package\n",
    "# Download all parts (will be around 3.5GB, hence not in this github repo)\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb169d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d1805b",
   "metadata": {},
   "source": [
    "# Tokenizing and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699eb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92fac7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"\"\"\n",
    "    hello computer, how are you today? \n",
    "    I would like to discuss apples and oranges.\n",
    "    Perhaps we can then move on to talking about stocks.\n",
    "    I like the stock.\n",
    "   $GME to the moon!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71739c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords are just useless information to the model, things like \"a, and, the\"\n",
    "# This is the process for stripping these words out of your sample data\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c71e765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33466c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'computer',\n",
       " ',',\n",
       " 'today',\n",
       " '?',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'discuss',\n",
       " 'apples',\n",
       " 'oranges',\n",
       " '.',\n",
       " 'Perhaps',\n",
       " 'move',\n",
       " 'talking',\n",
       " 'stocks',\n",
       " '.',\n",
       " 'I',\n",
       " 'like',\n",
       " 'stock',\n",
       " '.',\n",
       " '$',\n",
       " 'GME',\n",
       " 'moon',\n",
       " '!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [word for word in words if not word in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d7f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03dc40c3",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f49b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db47f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces words to their root\n",
    "# Things like WordNet actually solve this so that we won't need to do this\n",
    "# But it is always good to know where this stuff is coming from\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f915dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5610facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "038b211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f425b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15762688",
   "metadata": {},
   "source": [
    "# Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39484300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e9a9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag list:\n",
    "\n",
    "# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tadjective\t'big'\n",
    "# JJR\tadjective, comparative\t'bigger'\n",
    "# JJS\tadjective, superlative\t'biggest'\n",
    "# LS\tlist marker\t1)\n",
    "# MD\tmodal\tcould, will\n",
    "# NN\tnoun, singular 'desk'\n",
    "# NNS\tnoun plural\t'desks'\n",
    "# NNP\tproper noun, singular\t'Harrison'\n",
    "# NNPS\tproper noun, plural\t'Americans'\n",
    "# PDT\tpredeterminer\t'all the kids'\n",
    "# POS\tpossessive ending\tparent\\'s\n",
    "# PRP\tpersonal pronoun\tI, he, she\n",
    "# PRP$\tpossessive pronoun\tmy, his, hers\n",
    "# RB\tadverb\tvery, silently,\n",
    "# RBR\tadverb, comparative\tbetter\n",
    "# RBS\tadverb, superlative\tbest\n",
    "# RP\tparticle\tgive up\n",
    "# TO\tto\tgo 'to' the store.\n",
    "# UH\tinterjection\terrrrrrrrm\n",
    "# VB\tverb, base form\ttake\n",
    "# VBD\tverb, past tense\ttook\n",
    "# VBG\tverb, gerund/present participle\ttaking\n",
    "# VBN\tverb, past participle\ttaken\n",
    "# VBP\tverb, sing. present, non-3d\ttake\n",
    "# VBZ\tverb, 3rd person sing. present\ttakes\n",
    "# WDT\twh-determiner\twhich\n",
    "# WP\twh-pronoun\twho, what\n",
    "# WP$\tpossessive wh-pronoun\twhose\n",
    "# WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e33bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28523480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:10]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6ecdaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n",
      "[('31', 'CD'), (',', ','), ('2006', 'CD'), ('.', '.')]\n",
      "[('White', 'NNP'), ('House', 'NNP'), ('photo', 'NN'), ('by', 'IN'), ('Eric', 'NNP'), ('DraperEvery', 'NNP'), ('time', 'NN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('invited', 'JJ'), ('to', 'TO'), ('this', 'DT'), ('rostrum', 'NN'), (',', ','), ('I', 'PRP'), (\"'m\", 'VBP'), ('humbled', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('privilege', 'NN'), (',', ','), ('and', 'CC'), ('mindful', 'NN'), ('of', 'IN'), ('the', 'DT'), ('history', 'NN'), ('we', 'PRP'), (\"'ve\", 'VBP'), ('seen', 'VBN'), ('together', 'RB'), ('.', '.')]\n",
      "[('We', 'PRP'), ('have', 'VBP'), ('gathered', 'VBN'), ('under', 'IN'), ('this', 'DT'), ('Capitol', 'NNP'), ('dome', 'NN'), ('in', 'IN'), ('moments', 'NNS'), ('of', 'IN'), ('national', 'JJ'), ('mourning', 'NN'), ('and', 'CC'), ('national', 'JJ'), ('achievement', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('have', 'VBP'), ('served', 'VBN'), ('America', 'NNP'), ('through', 'IN'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('consequential', 'JJ'), ('periods', 'NNS'), ('of', 'IN'), ('our', 'PRP$'), ('history', 'NN'), ('--', ':'), ('and', 'CC'), ('it', 'PRP'), ('has', 'VBZ'), ('been', 'VBN'), ('my', 'PRP$'), ('honor', 'NN'), ('to', 'TO'), ('serve', 'VB'), ('with', 'IN'), ('you', 'PRP'), ('.', '.')]\n",
      "[('In', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('two', 'CD'), ('parties', 'NNS'), (',', ','), ('two', 'CD'), ('chambers', 'NNS'), (',', ','), ('and', 'CC'), ('two', 'CD'), ('elected', 'JJ'), ('branches', 'NNS'), (',', ','), ('there', 'EX'), ('will', 'MD'), ('always', 'RB'), ('be', 'VB'), ('differences', 'NNS'), ('and', 'CC'), ('debate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853482b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e036c2ee",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking in Natural Language Processing (NLP) is the process by which we group \n",
    "# various words together by their part of speech tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe0cba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:10]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c56ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ea47103dbc1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-b8eac7563f83>\u001b[0m in \u001b[0;36mprocess_content\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mchunked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunkParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ab617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67c8a283",
   "metadata": {},
   "source": [
    "# Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinking is a part of the chunking process with natural language processing with NLTK. \n",
    "# A chink is what we wish to remove from the chunk. \n",
    "# We define a chink in a very similar fashion compared to how we defined the chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b39e8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:10]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                                        }<VB.?|IN|DT>+{\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "634b1856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f092a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6edd83e",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition is useful to quickly find out what the subjects of discussion are. \n",
    "# NLTK comes packed full of options for us. \n",
    "# We can find just about any named entity, or we can look for specific ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f22c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NE Type and Examples\n",
    "\n",
    "# ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "# PERSON - Eddy Bonte, President Obama\n",
    "# LOCATION - Murray River, Mount Everest\n",
    "# DATE - June, 2008-06-29\n",
    "# TIME - two fifty a m, 1:30 p.m.\n",
    "# MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "# PERCENT - twenty pct, 18.75 %\n",
    "# FACILITY - Washington Monument, Stonehenge\n",
    "# GPE - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a58e0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:10]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            \n",
    "            namedEnt.draw()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "110e6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937586fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c72ed926",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcae070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very similar operation to stemming is called lemmatizing. \n",
    "# The major difference between these is, as you saw earlier, \n",
    "# stemming can often create non-existent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "124cc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f1a5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e21486aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "# Get root\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c59639f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "# Adjectives - pos = part of speech\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b2c3c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "bat\n"
     ]
    }
   ],
   "source": [
    "# Verbs\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"batting\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f0805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a0790c6",
   "metadata": {},
   "source": [
    "# NLTK Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded00f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bodies of texts. \n",
    "# Generally, corpora are grouped by some sort of defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e57a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9638ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "tokenized = sent_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6bfc1439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "1:5 And God called the light Day, and the darkness he called Night.\n",
      "And the evening and the morning were the first day.\n",
      "1:6 And God said, Let there be a firmament in the midst of the waters,\n",
      "and let it divide the waters from the waters.\n",
      "1:7 And God made the firmament, and divided the waters which were\n",
      "under the firmament from the waters which were above the firmament:\n",
      "and it was so.\n",
      "1:8 And God called the firmament Heaven.\n",
      "And the evening and the\n",
      "morning were the second day.\n",
      "1:9 And God said, Let the waters under the heaven be gathered together\n",
      "unto one place, and let the dry land appear: and it was so.\n",
      "1:10 And God called the dry land Earth; and the gathering together of\n",
      "the waters called he Seas: and God saw that it was good.\n",
      "1:11 And God said, Let the earth bring forth grass, the herb yielding\n",
      "seed, and the fruit tree yielding fruit after his kind, whose seed is\n",
      "in itself, upon the earth: and it was so.\n",
      "1:12 And the earth brought forth grass, and herb yielding seed after\n",
      "his kind, and the tree yielding fruit, whose seed was in itself, after\n",
      "his kind: and God saw that it was good.\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized[0:15]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6d96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "babd7cb0",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of the NLTK Corpora is WordNet. \n",
    "# I wouldn't totally classify WordNet as a Corpora, \n",
    "# if anything it is really a giant Lexicon, but, either way, it is super useful. \n",
    "# With WordNet we can do things like look up words and their meaning \n",
    "# according to their parts of speech, we can find synonyms, antonyms, \n",
    "# and even examples of the word in use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf077ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4341ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('plan.n.01')\n",
      "Synset('program.n.02')\n",
      "Synset('broadcast.n.02')\n",
      "Synset('platform.n.02')\n",
      "Synset('program.n.05')\n",
      "Synset('course_of_study.n.01')\n",
      "Synset('program.n.07')\n",
      "Synset('program.n.08')\n",
      "Synset('program.v.01')\n",
      "Synset('program.v.02')\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"program\")\n",
    "for syn in syns:\n",
    "    print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe3500dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('plan.n.01')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specific synset\n",
    "syns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f96e77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('plan.n.01.plan'),\n",
       " Lemma('plan.n.01.program'),\n",
       " Lemma('plan.n.01.programme')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All lemmas of that synset\n",
    "syns[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec5fdeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plan'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just the word of the synset\n",
    "syns[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b5d05211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a series of steps to be carried out or goals to be accomplished'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definition\n",
    "syns[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "844bacfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they drew up a six-step plan', 'they discussed plans for a new bond issue']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples\n",
    "syns[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f847e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms:\n",
      "just\n",
      "goodness\n",
      "honest\n",
      "honorable\n",
      "practiced\n",
      "adept\n",
      "undecomposed\n",
      "near\n",
      "thoroughly\n",
      "sound\n",
      "estimable\n",
      "dependable\n",
      "well\n",
      "good\n",
      "beneficial\n",
      "expert\n",
      "in_force\n",
      "salutary\n",
      "right\n",
      "skillful\n",
      "safe\n",
      "in_effect\n",
      "skilful\n",
      "ripe\n",
      "serious\n",
      "respectable\n",
      "unspoilt\n",
      "proficient\n",
      "commodity\n",
      "upright\n",
      "trade_good\n",
      "effective\n",
      "secure\n",
      "soundly\n",
      "dear\n",
      "unspoiled\n",
      "full\n",
      "\n",
      "Antonyms:\n",
      "ill\n",
      "bad\n",
      "evil\n",
      "evilness\n",
      "badness\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "\n",
    "print(\"Synonyms:\")\n",
    "for word in set(synonyms):\n",
    "    print(word)\n",
    "    \n",
    "print()\n",
    "print(\"Antonyms:\")\n",
    "for word in set(antonyms):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db7984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "965a47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "\n",
    "# wup = wu and palmer (semantic similarity)\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad83f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5cbb2fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8db4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b565341",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3bf3a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we understand some of the basics of of natural language processing \n",
    "# with the Python NLTK module, we're ready to try out text classification. \n",
    "# This is where we attempt to identify a body of text with some sort of label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37c692be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2762da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (list(movie_reviews.words(fileId)), category) \n",
    "    for category in movie_reviews.categories() \n",
    "    for fileId in movie_reviews.fileids(category)\n",
    "]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09e4202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f7f83f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('the', 76529),\n",
       " ('.', 65876),\n",
       " ('a', 38106),\n",
       " ('and', 35576),\n",
       " ('of', 34123),\n",
       " ('to', 31937),\n",
       " (\"'\", 30585),\n",
       " ('is', 25195),\n",
       " ('in', 21822),\n",
       " ('s', 18513),\n",
       " ('\"', 17612),\n",
       " ('it', 16107),\n",
       " ('that', 15924),\n",
       " ('-', 15595),\n",
       " (')', 11781),\n",
       " ('(', 11664),\n",
       " ('as', 11378),\n",
       " ('with', 10792),\n",
       " ('for', 9961)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8cdbc59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[\"stupid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba008f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49f69006",
   "metadata": {},
   "source": [
    "# Words as features for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our text classification, we have to find some way to \"describe\" bits of data, \n",
    "# which are labeled as either positive or negative for machine learning training purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [\n",
    "    (list(movie_reviews.words(fileId)), category) \n",
    "    for category in movie_reviews.categories() \n",
    "    for fileId in movie_reviews.fileids(category)\n",
    "]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc49574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "153702dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plot': True,\n",
       " ':': True,\n",
       " 'two': True,\n",
       " 'teen': True,\n",
       " 'couples': True,\n",
       " 'go': True,\n",
       " 'to': True,\n",
       " 'a': True,\n",
       " 'church': True,\n",
       " 'party': True,\n",
       " ',': True,\n",
       " 'drink': True,\n",
       " 'and': True,\n",
       " 'then': True,\n",
       " 'drive': True,\n",
       " '.': True,\n",
       " 'they': True,\n",
       " 'get': True,\n",
       " 'into': True,\n",
       " 'an': True,\n",
       " 'accident': True,\n",
       " 'one': True,\n",
       " 'of': True,\n",
       " 'the': True,\n",
       " 'guys': True,\n",
       " 'dies': True,\n",
       " 'but': True,\n",
       " 'his': True,\n",
       " 'girlfriend': True,\n",
       " 'continues': True,\n",
       " 'see': True,\n",
       " 'him': True,\n",
       " 'in': True,\n",
       " 'her': True,\n",
       " 'life': True,\n",
       " 'has': True,\n",
       " 'nightmares': True,\n",
       " 'what': True,\n",
       " \"'\": True,\n",
       " 's': True,\n",
       " 'deal': True,\n",
       " '?': True,\n",
       " 'watch': True,\n",
       " 'movie': True,\n",
       " '\"': True,\n",
       " 'sorta': True,\n",
       " 'find': True,\n",
       " 'out': True,\n",
       " 'critique': True,\n",
       " 'mind': True,\n",
       " '-': True,\n",
       " 'fuck': True,\n",
       " 'for': True,\n",
       " 'generation': True,\n",
       " 'that': True,\n",
       " 'touches': True,\n",
       " 'on': True,\n",
       " 'very': True,\n",
       " 'cool': True,\n",
       " 'idea': True,\n",
       " 'presents': True,\n",
       " 'it': True,\n",
       " 'bad': True,\n",
       " 'package': True,\n",
       " 'which': True,\n",
       " 'is': True,\n",
       " 'makes': True,\n",
       " 'this': True,\n",
       " 'review': True,\n",
       " 'even': True,\n",
       " 'harder': True,\n",
       " 'write': True,\n",
       " 'since': True,\n",
       " 'i': True,\n",
       " 'generally': True,\n",
       " 'applaud': True,\n",
       " 'films': True,\n",
       " 'attempt': True,\n",
       " 'break': True,\n",
       " 'mold': True,\n",
       " 'mess': True,\n",
       " 'with': True,\n",
       " 'your': True,\n",
       " 'head': True,\n",
       " 'such': True,\n",
       " '(': True,\n",
       " 'lost': True,\n",
       " 'highway': True,\n",
       " '&': True,\n",
       " 'memento': True,\n",
       " ')': True,\n",
       " 'there': True,\n",
       " 'are': True,\n",
       " 'good': True,\n",
       " 'ways': True,\n",
       " 'making': True,\n",
       " 'all': True,\n",
       " 'types': True,\n",
       " 'these': True,\n",
       " 'folks': True,\n",
       " 'just': True,\n",
       " 'didn': True,\n",
       " 't': True,\n",
       " 'snag': True,\n",
       " 'correctly': True,\n",
       " 'seem': True,\n",
       " 'have': True,\n",
       " 'taken': True,\n",
       " 'pretty': True,\n",
       " 'neat': True,\n",
       " 'concept': True,\n",
       " 'executed': True,\n",
       " 'terribly': True,\n",
       " 'so': True,\n",
       " 'problems': True,\n",
       " 'well': True,\n",
       " 'its': True,\n",
       " 'main': True,\n",
       " 'problem': True,\n",
       " 'simply': True,\n",
       " 'too': True,\n",
       " 'jumbled': True,\n",
       " 'starts': True,\n",
       " 'off': True,\n",
       " 'normal': True,\n",
       " 'downshifts': True,\n",
       " 'fantasy': True,\n",
       " 'world': True,\n",
       " 'you': True,\n",
       " 'as': True,\n",
       " 'audience': True,\n",
       " 'member': True,\n",
       " 'no': True,\n",
       " 'going': True,\n",
       " 'dreams': True,\n",
       " 'characters': True,\n",
       " 'coming': True,\n",
       " 'back': True,\n",
       " 'from': True,\n",
       " 'dead': True,\n",
       " 'others': True,\n",
       " 'who': True,\n",
       " 'look': True,\n",
       " 'like': True,\n",
       " 'strange': True,\n",
       " 'apparitions': True,\n",
       " 'disappearances': True,\n",
       " 'looooot': True,\n",
       " 'chase': True,\n",
       " 'scenes': True,\n",
       " 'tons': True,\n",
       " 'weird': True,\n",
       " 'things': True,\n",
       " 'happen': True,\n",
       " 'most': True,\n",
       " 'not': True,\n",
       " 'explained': True,\n",
       " 'now': True,\n",
       " 'personally': True,\n",
       " 'don': True,\n",
       " 'trying': True,\n",
       " 'unravel': True,\n",
       " 'film': True,\n",
       " 'every': True,\n",
       " 'when': True,\n",
       " 'does': True,\n",
       " 'give': True,\n",
       " 'me': True,\n",
       " 'same': True,\n",
       " 'clue': True,\n",
       " 'over': True,\n",
       " 'again': True,\n",
       " 'kind': True,\n",
       " 'fed': True,\n",
       " 'up': True,\n",
       " 'after': True,\n",
       " 'while': True,\n",
       " 'biggest': True,\n",
       " 'obviously': True,\n",
       " 'got': True,\n",
       " 'big': True,\n",
       " 'secret': True,\n",
       " 'hide': True,\n",
       " 'seems': True,\n",
       " 'want': True,\n",
       " 'completely': True,\n",
       " 'until': True,\n",
       " 'final': True,\n",
       " 'five': True,\n",
       " 'minutes': True,\n",
       " 'do': True,\n",
       " 'make': True,\n",
       " 'entertaining': True,\n",
       " 'thrilling': True,\n",
       " 'or': True,\n",
       " 'engaging': True,\n",
       " 'meantime': True,\n",
       " 'really': True,\n",
       " 'sad': True,\n",
       " 'part': True,\n",
       " 'arrow': True,\n",
       " 'both': True,\n",
       " 'dig': True,\n",
       " 'flicks': True,\n",
       " 'we': True,\n",
       " 'actually': True,\n",
       " 'figured': True,\n",
       " 'by': True,\n",
       " 'half': True,\n",
       " 'way': True,\n",
       " 'point': True,\n",
       " 'strangeness': True,\n",
       " 'did': True,\n",
       " 'start': True,\n",
       " 'little': True,\n",
       " 'bit': True,\n",
       " 'sense': True,\n",
       " 'still': True,\n",
       " 'more': True,\n",
       " 'guess': True,\n",
       " 'bottom': True,\n",
       " 'line': True,\n",
       " 'movies': True,\n",
       " 'should': True,\n",
       " 'always': True,\n",
       " 'sure': True,\n",
       " 'before': True,\n",
       " 'given': True,\n",
       " 'password': True,\n",
       " 'enter': True,\n",
       " 'understanding': True,\n",
       " 'mean': True,\n",
       " 'showing': True,\n",
       " 'melissa': True,\n",
       " 'sagemiller': True,\n",
       " 'running': True,\n",
       " 'away': True,\n",
       " 'visions': True,\n",
       " 'about': True,\n",
       " '20': True,\n",
       " 'throughout': True,\n",
       " 'plain': True,\n",
       " 'lazy': True,\n",
       " '!': True,\n",
       " 'okay': True,\n",
       " 'people': True,\n",
       " 'chasing': True,\n",
       " 'know': True,\n",
       " 'need': True,\n",
       " 'how': True,\n",
       " 'giving': True,\n",
       " 'us': True,\n",
       " 'different': True,\n",
       " 'offering': True,\n",
       " 'further': True,\n",
       " 'insight': True,\n",
       " 'down': True,\n",
       " 'apparently': True,\n",
       " 'studio': True,\n",
       " 'took': True,\n",
       " 'director': True,\n",
       " 'chopped': True,\n",
       " 'themselves': True,\n",
       " 'shows': True,\n",
       " 'might': True,\n",
       " 've': True,\n",
       " 'been': True,\n",
       " 'decent': True,\n",
       " 'here': True,\n",
       " 'somewhere': True,\n",
       " 'suits': True,\n",
       " 'decided': True,\n",
       " 'turning': True,\n",
       " 'music': True,\n",
       " 'video': True,\n",
       " 'edge': True,\n",
       " 'would': True,\n",
       " 'actors': True,\n",
       " 'although': True,\n",
       " 'wes': True,\n",
       " 'bentley': True,\n",
       " 'seemed': True,\n",
       " 'be': True,\n",
       " 'playing': True,\n",
       " 'exact': True,\n",
       " 'character': True,\n",
       " 'he': True,\n",
       " 'american': True,\n",
       " 'beauty': True,\n",
       " 'only': True,\n",
       " 'new': True,\n",
       " 'neighborhood': True,\n",
       " 'my': True,\n",
       " 'kudos': True,\n",
       " 'holds': True,\n",
       " 'own': True,\n",
       " 'entire': True,\n",
       " 'feeling': True,\n",
       " 'unraveling': True,\n",
       " 'overall': True,\n",
       " 'doesn': True,\n",
       " 'stick': True,\n",
       " 'because': True,\n",
       " 'entertain': True,\n",
       " 'confusing': True,\n",
       " 'rarely': True,\n",
       " 'excites': True,\n",
       " 'feels': True,\n",
       " 'redundant': True,\n",
       " 'runtime': True,\n",
       " 'despite': True,\n",
       " 'ending': True,\n",
       " 'explanation': True,\n",
       " 'craziness': True,\n",
       " 'came': True,\n",
       " 'oh': True,\n",
       " 'horror': True,\n",
       " 'slasher': True,\n",
       " 'flick': True,\n",
       " 'packaged': True,\n",
       " 'someone': True,\n",
       " 'assuming': True,\n",
       " 'genre': True,\n",
       " 'hot': True,\n",
       " 'kids': True,\n",
       " 'also': True,\n",
       " 'wrapped': True,\n",
       " 'production': True,\n",
       " 'years': True,\n",
       " 'ago': True,\n",
       " 'sitting': True,\n",
       " 'shelves': True,\n",
       " 'ever': True,\n",
       " 'whatever': True,\n",
       " 'skip': True,\n",
       " 'where': True,\n",
       " 'joblo': True,\n",
       " 'nightmare': True,\n",
       " 'elm': True,\n",
       " 'street': True,\n",
       " '3': True,\n",
       " '7': True,\n",
       " '/': True,\n",
       " '10': True,\n",
       " 'blair': True,\n",
       " 'witch': True,\n",
       " '2': True,\n",
       " 'crow': True,\n",
       " '9': True,\n",
       " 'salvation': True,\n",
       " '4': True,\n",
       " 'stir': True,\n",
       " 'echoes': True,\n",
       " '8': True,\n",
       " 'happy': False,\n",
       " 'bastard': False,\n",
       " 'quick': False,\n",
       " 'damn': False,\n",
       " 'y2k': False,\n",
       " 'bug': False,\n",
       " 'starring': False,\n",
       " 'jamie': False,\n",
       " 'lee': False,\n",
       " 'curtis': False,\n",
       " 'another': False,\n",
       " 'baldwin': False,\n",
       " 'brother': False,\n",
       " 'william': False,\n",
       " 'time': False,\n",
       " 'story': False,\n",
       " 'regarding': False,\n",
       " 'crew': False,\n",
       " 'tugboat': False,\n",
       " 'comes': False,\n",
       " 'across': False,\n",
       " 'deserted': False,\n",
       " 'russian': False,\n",
       " 'tech': False,\n",
       " 'ship': False,\n",
       " 'kick': False,\n",
       " 'power': False,\n",
       " 'within': False,\n",
       " 'gore': False,\n",
       " 'bringing': False,\n",
       " 'few': False,\n",
       " 'action': False,\n",
       " 'sequences': False,\n",
       " 'virus': False,\n",
       " 'empty': False,\n",
       " 'flash': False,\n",
       " 'substance': False,\n",
       " 'why': False,\n",
       " 'was': False,\n",
       " 'middle': False,\n",
       " 'nowhere': False,\n",
       " 'origin': False,\n",
       " 'pink': False,\n",
       " 'flashy': False,\n",
       " 'thing': False,\n",
       " 'hit': False,\n",
       " 'mir': False,\n",
       " 'course': False,\n",
       " 'donald': False,\n",
       " 'sutherland': False,\n",
       " 'stumbling': False,\n",
       " 'around': False,\n",
       " 'drunkenly': False,\n",
       " 'hey': False,\n",
       " 'let': False,\n",
       " 'some': False,\n",
       " 'robots': False,\n",
       " 'acting': False,\n",
       " 'below': False,\n",
       " 'average': False,\n",
       " 'likes': False,\n",
       " 're': False,\n",
       " 'likely': False,\n",
       " 'work': False,\n",
       " 'halloween': False,\n",
       " 'h20': False,\n",
       " 'wasted': False,\n",
       " 'real': False,\n",
       " 'star': False,\n",
       " 'stan': False,\n",
       " 'winston': False,\n",
       " 'robot': False,\n",
       " 'design': False,\n",
       " 'schnazzy': False,\n",
       " 'cgi': False,\n",
       " 'occasional': False,\n",
       " 'shot': False,\n",
       " 'picking': False,\n",
       " 'brain': False,\n",
       " 'if': False,\n",
       " 'body': False,\n",
       " 'parts': False,\n",
       " 'turn': False,\n",
       " 'otherwise': False,\n",
       " 'much': False,\n",
       " 'sunken': False,\n",
       " 'jaded': False,\n",
       " 'viewer': False,\n",
       " 'thankful': False,\n",
       " 'invention': False,\n",
       " 'timex': False,\n",
       " 'indiglo': False,\n",
       " 'based': False,\n",
       " 'late': False,\n",
       " '1960': False,\n",
       " 'television': False,\n",
       " 'show': False,\n",
       " 'name': False,\n",
       " 'mod': False,\n",
       " 'squad': False,\n",
       " 'tells': False,\n",
       " 'tale': False,\n",
       " 'three': False,\n",
       " 'reformed': False,\n",
       " 'criminals': False,\n",
       " 'under': False,\n",
       " 'employ': False,\n",
       " 'police': False,\n",
       " 'undercover': False,\n",
       " 'however': False,\n",
       " 'wrong': False,\n",
       " 'evidence': False,\n",
       " 'gets': False,\n",
       " 'stolen': False,\n",
       " 'immediately': False,\n",
       " 'suspicion': False,\n",
       " 'ads': False,\n",
       " 'cuts': False,\n",
       " 'claire': False,\n",
       " 'dane': False,\n",
       " 'nice': False,\n",
       " 'hair': False,\n",
       " 'cute': False,\n",
       " 'outfits': False,\n",
       " 'car': False,\n",
       " 'chases': False,\n",
       " 'stuff': False,\n",
       " 'blowing': False,\n",
       " 'sounds': False,\n",
       " 'first': False,\n",
       " 'fifteen': False,\n",
       " 'quickly': False,\n",
       " 'becomes': False,\n",
       " 'apparent': False,\n",
       " 'certainly': False,\n",
       " 'slick': False,\n",
       " 'looking': False,\n",
       " 'complete': False,\n",
       " 'costumes': False,\n",
       " 'isn': False,\n",
       " 'enough': False,\n",
       " 'best': False,\n",
       " 'described': False,\n",
       " 'cross': False,\n",
       " 'between': False,\n",
       " 'hour': False,\n",
       " 'long': False,\n",
       " 'cop': False,\n",
       " 'stretched': False,\n",
       " 'span': False,\n",
       " 'single': False,\n",
       " 'clich': False,\n",
       " 'matter': False,\n",
       " 'elements': False,\n",
       " 'recycled': False,\n",
       " 'everything': False,\n",
       " 'already': False,\n",
       " 'seen': False,\n",
       " 'nothing': False,\n",
       " 'spectacular': False,\n",
       " 'sometimes': False,\n",
       " 'bordering': False,\n",
       " 'wooden': False,\n",
       " 'danes': False,\n",
       " 'omar': False,\n",
       " 'epps': False,\n",
       " 'deliver': False,\n",
       " 'their': False,\n",
       " 'lines': False,\n",
       " 'bored': False,\n",
       " 'transfers': False,\n",
       " 'onto': False,\n",
       " 'escape': False,\n",
       " 'relatively': False,\n",
       " 'unscathed': False,\n",
       " 'giovanni': False,\n",
       " 'ribisi': False,\n",
       " 'plays': False,\n",
       " 'resident': False,\n",
       " 'crazy': False,\n",
       " 'man': False,\n",
       " 'ultimately': False,\n",
       " 'being': False,\n",
       " 'worth': False,\n",
       " 'watching': False,\n",
       " 'unfortunately': False,\n",
       " 'save': False,\n",
       " 'convoluted': False,\n",
       " 'apart': False,\n",
       " 'occupying': False,\n",
       " 'screen': False,\n",
       " 'young': False,\n",
       " 'cast': False,\n",
       " 'clothes': False,\n",
       " 'hip': False,\n",
       " 'soundtrack': False,\n",
       " 'appears': False,\n",
       " 'geared': False,\n",
       " 'towards': False,\n",
       " 'teenage': False,\n",
       " 'mindset': False,\n",
       " 'r': False,\n",
       " 'rating': False,\n",
       " 'content': False,\n",
       " 'justify': False,\n",
       " 'juvenile': False,\n",
       " 'older': False,\n",
       " 'information': False,\n",
       " 'literally': False,\n",
       " 'spoon': False,\n",
       " 'hard': False,\n",
       " 'instead': False,\n",
       " 'telling': False,\n",
       " 'dialogue': False,\n",
       " 'poorly': False,\n",
       " 'written': False,\n",
       " 'extremely': False,\n",
       " 'predictable': False,\n",
       " 'progresses': False,\n",
       " 'won': False,\n",
       " 'care': False,\n",
       " 'heroes': False,\n",
       " 'any': False,\n",
       " 'jeopardy': False,\n",
       " 'll': False,\n",
       " 'aren': False,\n",
       " 'basing': False,\n",
       " 'nobody': False,\n",
       " 'remembers': False,\n",
       " 'questionable': False,\n",
       " 'wisdom': False,\n",
       " 'especially': False,\n",
       " 'considers': False,\n",
       " 'target': False,\n",
       " 'fact': False,\n",
       " 'number': False,\n",
       " 'memorable': False,\n",
       " 'can': False,\n",
       " 'counted': False,\n",
       " 'hand': False,\n",
       " 'missing': False,\n",
       " 'finger': False,\n",
       " 'times': False,\n",
       " 'checked': False,\n",
       " 'six': False,\n",
       " 'clear': False,\n",
       " 'indication': False,\n",
       " 'them': False,\n",
       " 'than': False,\n",
       " 'cash': False,\n",
       " 'spending': False,\n",
       " 'dollar': False,\n",
       " 'judging': False,\n",
       " 'rash': False,\n",
       " 'awful': False,\n",
       " 'seeing': False,\n",
       " 'avoid': False,\n",
       " 'at': False,\n",
       " 'costs': False,\n",
       " 'quest': False,\n",
       " 'camelot': False,\n",
       " 'warner': False,\n",
       " 'bros': False,\n",
       " 'feature': False,\n",
       " 'length': False,\n",
       " 'fully': False,\n",
       " 'animated': False,\n",
       " 'steal': False,\n",
       " 'clout': False,\n",
       " 'disney': False,\n",
       " 'cartoon': False,\n",
       " 'empire': False,\n",
       " 'mouse': False,\n",
       " 'reason': False,\n",
       " 'worried': False,\n",
       " 'other': False,\n",
       " 'recent': False,\n",
       " 'challenger': False,\n",
       " 'throne': False,\n",
       " 'last': False,\n",
       " 'fall': False,\n",
       " 'promising': False,\n",
       " 'flawed': False,\n",
       " '20th': False,\n",
       " 'century': False,\n",
       " 'fox': False,\n",
       " 'anastasia': False,\n",
       " 'hercules': False,\n",
       " 'lively': False,\n",
       " 'colorful': False,\n",
       " 'palate': False,\n",
       " 'had': False,\n",
       " 'beat': False,\n",
       " 'hands': False,\n",
       " 'crown': False,\n",
       " '1997': False,\n",
       " 'piece': False,\n",
       " 'animation': False,\n",
       " 'year': False,\n",
       " 'contest': False,\n",
       " 'arrival': False,\n",
       " 'magic': False,\n",
       " 'kingdom': False,\n",
       " 'mediocre': False,\n",
       " '--': False,\n",
       " 'd': False,\n",
       " 'pocahontas': False,\n",
       " 'those': False,\n",
       " 'keeping': False,\n",
       " 'score': False,\n",
       " 'nearly': False,\n",
       " 'dull': False,\n",
       " 'revolves': False,\n",
       " 'adventures': False,\n",
       " 'free': False,\n",
       " 'spirited': False,\n",
       " 'kayley': False,\n",
       " 'voiced': False,\n",
       " 'jessalyn': False,\n",
       " 'gilsig': False,\n",
       " 'early': False,\n",
       " 'daughter': False,\n",
       " 'belated': False,\n",
       " 'knight': False,\n",
       " 'king': False,\n",
       " 'arthur': False,\n",
       " 'round': False,\n",
       " 'table': False,\n",
       " 'dream': False,\n",
       " 'follow': False,\n",
       " 'father': False,\n",
       " 'footsteps': False,\n",
       " 'she': False,\n",
       " 'chance': False,\n",
       " 'evil': False,\n",
       " 'warlord': False,\n",
       " 'ruber': False,\n",
       " 'gary': False,\n",
       " 'oldman': False,\n",
       " 'ex': False,\n",
       " 'gone': False,\n",
       " 'steals': False,\n",
       " 'magical': False,\n",
       " 'sword': False,\n",
       " 'excalibur': False,\n",
       " 'accidentally': False,\n",
       " 'loses': False,\n",
       " 'dangerous': False,\n",
       " 'booby': False,\n",
       " 'trapped': False,\n",
       " 'forest': False,\n",
       " 'help': False,\n",
       " 'hunky': False,\n",
       " 'blind': False,\n",
       " 'timberland': False,\n",
       " 'dweller': False,\n",
       " 'garrett': False,\n",
       " 'carey': False,\n",
       " 'elwes': False,\n",
       " 'headed': False,\n",
       " 'dragon': False,\n",
       " 'eric': False,\n",
       " 'idle': False,\n",
       " 'rickles': False,\n",
       " 'arguing': False,\n",
       " 'itself': False,\n",
       " 'able': False,\n",
       " 'medieval': False,\n",
       " 'sexist': False,\n",
       " 'prove': False,\n",
       " 'fighter': False,\n",
       " 'side': False,\n",
       " 'pure': False,\n",
       " 'showmanship': False,\n",
       " 'essential': False,\n",
       " 'element': False,\n",
       " 'expected': False,\n",
       " 'climb': False,\n",
       " 'high': False,\n",
       " 'ranks': False,\n",
       " 'differentiates': False,\n",
       " 'something': False,\n",
       " 'saturday': False,\n",
       " 'morning': False,\n",
       " 'subpar': False,\n",
       " 'instantly': False,\n",
       " 'forgettable': False,\n",
       " 'songs': False,\n",
       " 'integrated': False,\n",
       " 'computerized': False,\n",
       " 'footage': False,\n",
       " 'compare': False,\n",
       " 'run': False,\n",
       " 'angry': False,\n",
       " 'ogre': False,\n",
       " 'herc': False,\n",
       " 'battle': False,\n",
       " 'hydra': False,\n",
       " 'rest': False,\n",
       " 'case': False,\n",
       " 'stink': False,\n",
       " 'none': False,\n",
       " 'remotely': False,\n",
       " 'interesting': False,\n",
       " 'race': False,\n",
       " 'bland': False,\n",
       " 'end': False,\n",
       " 'tie': False,\n",
       " 'win': False,\n",
       " 'comedy': False,\n",
       " 'shtick': False,\n",
       " 'awfully': False,\n",
       " 'cloying': False,\n",
       " 'least': False,\n",
       " 'signs': False,\n",
       " 'pulse': False,\n",
       " 'fans': False,\n",
       " \"-'\": False,\n",
       " '90s': False,\n",
       " 'tgif': False,\n",
       " 'will': False,\n",
       " 'thrilled': False,\n",
       " 'jaleel': False,\n",
       " 'urkel': False,\n",
       " 'white': False,\n",
       " 'bronson': False,\n",
       " 'balki': False,\n",
       " 'pinchot': False,\n",
       " 'sharing': False,\n",
       " 'nicely': False,\n",
       " 'realized': False,\n",
       " 'though': False,\n",
       " 'm': False,\n",
       " 'loss': False,\n",
       " 'recall': False,\n",
       " 'specific': False,\n",
       " 'providing': False,\n",
       " 'voice': False,\n",
       " 'talent': False,\n",
       " 'enthusiastic': False,\n",
       " 'paired': False,\n",
       " 'singers': False,\n",
       " 'sound': False,\n",
       " 'musical': False,\n",
       " 'moments': False,\n",
       " 'jane': False,\n",
       " 'seymour': False,\n",
       " 'celine': False,\n",
       " 'dion': False,\n",
       " 'must': False,\n",
       " 'strain': False,\n",
       " 'through': False,\n",
       " 'aside': False,\n",
       " 'children': False,\n",
       " 'probably': False,\n",
       " 'adults': False,\n",
       " 'grievous': False,\n",
       " 'error': False,\n",
       " 'lack': False,\n",
       " 'personality': False,\n",
       " 'learn': False,\n",
       " 'goes': False,\n",
       " 'synopsis': False,\n",
       " 'mentally': False,\n",
       " 'unstable': False,\n",
       " 'undergoing': False,\n",
       " 'psychotherapy': False,\n",
       " 'saves': False,\n",
       " 'boy': False,\n",
       " 'potentially': False,\n",
       " 'fatal': False,\n",
       " 'falls': False,\n",
       " 'love': False,\n",
       " 'mother': False,\n",
       " 'fledgling': False,\n",
       " 'restauranteur': False,\n",
       " 'unsuccessfully': False,\n",
       " 'attempting': False,\n",
       " 'gain': False,\n",
       " 'woman': False,\n",
       " 'favor': False,\n",
       " 'takes': False,\n",
       " 'pictures': False,\n",
       " 'kills': False,\n",
       " 'comments': False,\n",
       " 'stalked': False,\n",
       " 'yet': False,\n",
       " 'seemingly': False,\n",
       " 'endless': False,\n",
       " 'string': False,\n",
       " 'spurned': False,\n",
       " 'psychos': False,\n",
       " 'getting': False,\n",
       " 'revenge': False,\n",
       " 'type': False,\n",
       " 'stable': False,\n",
       " 'category': False,\n",
       " '1990s': False,\n",
       " 'industry': False,\n",
       " 'theatrical': False,\n",
       " 'direct': False,\n",
       " 'proliferation': False,\n",
       " 'may': False,\n",
       " 'due': False,\n",
       " 'typically': False,\n",
       " 'inexpensive': False,\n",
       " 'produce': False,\n",
       " 'special': False,\n",
       " 'effects': False,\n",
       " 'stars': False,\n",
       " 'serve': False,\n",
       " 'vehicles': False,\n",
       " 'nudity': False,\n",
       " 'allowing': False,\n",
       " 'frequent': False,\n",
       " 'night': False,\n",
       " 'cable': False,\n",
       " 'wavers': False,\n",
       " 'slightly': False,\n",
       " 'norm': False,\n",
       " 'respect': False,\n",
       " 'psycho': False,\n",
       " 'never': False,\n",
       " 'affair': False,\n",
       " ';': False,\n",
       " 'contrary': False,\n",
       " 'rejected': False,\n",
       " 'rather': False,\n",
       " 'lover': False,\n",
       " 'wife': False,\n",
       " 'husband': False,\n",
       " 'entry': False,\n",
       " 'doomed': False,\n",
       " 'collect': False,\n",
       " 'dust': False,\n",
       " 'viewed': False,\n",
       " 'midnight': False,\n",
       " 'provide': False,\n",
       " 'suspense': False,\n",
       " 'sets': False,\n",
       " 'interspersed': False,\n",
       " 'opening': False,\n",
       " 'credits': False,\n",
       " 'instance': False,\n",
       " 'serious': False,\n",
       " 'sounding': False,\n",
       " 'narrator': False,\n",
       " 'spouts': False,\n",
       " 'statistics': False,\n",
       " 'stalkers': False,\n",
       " 'ponders': False,\n",
       " 'cause': False,\n",
       " 'stalk': False,\n",
       " 'implicitly': False,\n",
       " 'implied': False,\n",
       " 'men': False,\n",
       " 'shown': False,\n",
       " 'snapshot': False,\n",
       " 'actor': False,\n",
       " 'jay': False,\n",
       " 'underwood': False,\n",
       " 'states': False,\n",
       " 'daryl': False,\n",
       " 'gleason': False,\n",
       " 'stalker': False,\n",
       " 'brooke': False,\n",
       " 'daniels': False,\n",
       " 'meant': False,\n",
       " 'called': False,\n",
       " 'guesswork': False,\n",
       " 'required': False,\n",
       " 'proceeds': False,\n",
       " 'begins': False,\n",
       " 'obvious': False,\n",
       " 'sequence': False,\n",
       " 'contrived': False,\n",
       " 'quite': False,\n",
       " 'brings': False,\n",
       " 'victim': False,\n",
       " 'together': False,\n",
       " 'obsesses': False,\n",
       " 'follows': False,\n",
       " 'tries': False,\n",
       " 'woo': False,\n",
       " 'plans': False,\n",
       " 'become': False,\n",
       " 'desperate': False,\n",
       " 'elaborate': False,\n",
       " 'include': False,\n",
       " 'cliche': False,\n",
       " 'murdered': False,\n",
       " 'pet': False,\n",
       " 'require': False,\n",
       " 'found': False,\n",
       " 'exception': False,\n",
       " 'cat': False,\n",
       " 'shower': False,\n",
       " 'events': False,\n",
       " 'lead': False,\n",
       " 'inevitable': False,\n",
       " 'showdown': False,\n",
       " 'survives': False,\n",
       " 'invariably': False,\n",
       " 'conclusion': False,\n",
       " 'turkey': False,\n",
       " 'uniformly': False,\n",
       " 'adequate': False,\n",
       " 'anything': False,\n",
       " 'home': False,\n",
       " 'either': False,\n",
       " 'turns': False,\n",
       " 'toward': False,\n",
       " 'melodrama': False,\n",
       " 'overdoes': False,\n",
       " 'words': False,\n",
       " 'manages': False,\n",
       " 'creepy': False,\n",
       " 'pass': False,\n",
       " 'demands': False,\n",
       " 'maryam': False,\n",
       " 'abo': False,\n",
       " 'close': False,\n",
       " 'played': False,\n",
       " 'bond': False,\n",
       " 'chick': False,\n",
       " 'living': False,\n",
       " 'daylights': False,\n",
       " 'equally': False,\n",
       " 'title': False,\n",
       " 'ditzy': False,\n",
       " 'strong': False,\n",
       " 'independent': False,\n",
       " 'business': False,\n",
       " 'owner': False,\n",
       " 'needs': False,\n",
       " 'proceed': False,\n",
       " 'example': False,\n",
       " 'suspicions': False,\n",
       " 'ensure': False,\n",
       " 'use': False,\n",
       " 'excuse': False,\n",
       " 'decides': False,\n",
       " 'return': False,\n",
       " 'toolbox': False,\n",
       " 'left': False,\n",
       " 'place': False,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_features(movie_reviews.words(\"neg/cv000_29416.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "034c89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c7493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e517db0",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm of choice, at least at a basic level, for text analysis is often the Naive Bayes classifier. \n",
    "# Part of the reason for this is that text data is almost always massive in size. \n",
    "# The Naive Bayes algorithm is so simple that it can be used at scale very easily with minimal process requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [\n",
    "    (list(movie_reviews.words(fileId)), category) \n",
    "    for category in movie_reviews.categories() \n",
    "    for fileId in movie_reviews.fileids(category)\n",
    "]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18daae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "training_set = feature_sets[:1900]\n",
    "testing_set = feature_sets[1900:]\n",
    "\n",
    "# We are going to use a Naive Bayes approach\n",
    "# posterior = prior occurences * likelihood / evidence\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea90430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy: 78.00 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Naive Bayes Algo accuracy: {nltk.classify.accuracy(classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eabe069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   sucks = True              neg : pos    =     17.2 : 1.0\n",
      "                  annual = True              pos : neg    =      9.6 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.4 : 1.0\n",
      "                 frances = True              pos : neg    =      8.9 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.7 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.1 : 1.0\n",
      "                  shoddy = True              neg : pos    =      7.1 : 1.0\n",
      "           unimaginative = True              neg : pos    =      7.1 : 1.0\n",
      "                  turkey = True              neg : pos    =      6.8 : 1.0\n",
      "                  regard = True              pos : neg    =      6.5 : 1.0\n",
      "                 kidding = True              neg : pos    =      6.4 : 1.0\n",
      "                 cunning = True              pos : neg    =      6.3 : 1.0\n",
      "                obstacle = True              pos : neg    =      6.3 : 1.0\n",
      "                 singers = True              pos : neg    =      6.3 : 1.0\n",
      "                unspoken = True              pos : neg    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56464ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e7f94f0",
   "metadata": {},
   "source": [
    "# Save Classifier with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd87d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [\n",
    "    (list(movie_reviews.words(fileId)), category) \n",
    "    for category in movie_reviews.categories() \n",
    "    for fileId in movie_reviews.fileids(category)\n",
    "]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "training_set = feature_sets[:1900]\n",
    "testing_set = feature_sets[1900:]\n",
    "\n",
    "# We are going to use a Naive Bayes approach\n",
    "# posterior = prior occurences * likelihood / evidence\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06125618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb370b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = open(\"naive_bayes.pickle\", \"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91958253",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open(\"naive_bayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79190436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy: 84.00 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Naive Bayes Algo accuracy: {nltk.classify.accuracy(classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33787845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc732894",
   "metadata": {},
   "source": [
    "# Scikit-Learn incorporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc161548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despite coming packed with some classifiers, \n",
    "# NLTK is mainly a toolkit focused on natural language processing, \n",
    "# and not machine learning specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "159d1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d8d07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (list(movie_reviews.words(fileId)), category) \n",
    "    for category in movie_reviews.categories() \n",
    "    for fileId in movie_reviews.fileids(category)\n",
    "]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "training_set = feature_sets[:1900]\n",
    "testing_set = feature_sets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5da748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy: 81.00 %\n"
     ]
    }
   ],
   "source": [
    "original_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(f\"Original Naive Bayes Algo accuracy: {nltk.classify.accuracy(original_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84b5c38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy: 80.00 %\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(f\"MNB_classifier accuracy: {nltk.classify.accuracy(MNB_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45e4f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoulli_classifier accuracy: 81.00 %\n"
     ]
    }
   ],
   "source": [
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(training_set)\n",
    "print(f\"bernoulli_classifier accuracy: {nltk.classify.accuracy(bernoulli_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39aae123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian_classifier = SklearnClassifier(GaussianNB())\n",
    "# gaussian_classifier.train(training_set)\n",
    "# print(f\"GaussianNB Naive Bayes Algo accuracy: {nltk.classify.accuracy(gaussian_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76b43314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomm/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy: 85.00 %\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(f\"LogisticRegression_classifier accuracy: {nltk.classify.accuracy(LogisticRegression_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9957f46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy: 83.00 %\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(f\"SGDClassifier_classifier accuracy: {nltk.classify.accuracy(SGDClassifier_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d833d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy: 88.00 %\n"
     ]
    }
   ],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(f\"SVC_classifier accuracy: {nltk.classify.accuracy(SVC_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04637286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy: 83.00 %\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(f\"LinearSVC_classifier accuracy: {nltk.classify.accuracy(LinearSVC_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aefd3e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy: 86.00 %\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(f\"NuSVC_classifier accuracy: {nltk.classify.accuracy(NuSVC_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214616e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5558d3c",
   "metadata": {},
   "source": [
    "# Combining Algos with a Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d757f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomm/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(NuSVC())>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "documents = [\n",
    "    (list(movie_reviews.words(fileId)), category) \n",
    "    for category in movie_reviews.categories() \n",
    "    for fileId in movie_reviews.fileids(category)\n",
    "]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "training_set = feature_sets[:1900]\n",
    "testing_set = feature_sets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "72fcf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        \n",
    "        return conf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "159baede",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(f\"Original Naive Bayes Algo accuracy: {nltk.classify.accuracy(original_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(f\"MNB_classifier accuracy: {nltk.classify.accuracy(MNB_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(training_set)\n",
    "print(f\"bernoulli_classifier accuracy: {nltk.classify.accuracy(bernoulli_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(f\"LogisticRegression_classifier accuracy: {nltk.classify.accuracy(LogisticRegression_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(f\"SGDClassifier_classifier accuracy: {nltk.classify.accuracy(SGDClassifier_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(f\"LinearSVC_classifier accuracy: {nltk.classify.accuracy(LinearSVC_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(f\"NuSVC_classifier accuracy: {nltk.classify.accuracy(NuSVC_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "23aa295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy: 77.00 %\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(\n",
    "    original_classifier,\n",
    "    MNB_classifier,\n",
    "    bernoulli_classifier,\n",
    "    LogisticRegression_classifier,\n",
    "    SGDClassifier_classifier,\n",
    "    LinearSVC_classifier,\n",
    "    NuSVC_classifier,\n",
    ")\n",
    "\n",
    "print(f\"voted_classifier accuracy: {nltk.classify.accuracy(voted_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d387bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: neg\n",
      "Confidence: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "target_review = testing_set[0][0]\n",
    "print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d99e1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: neg\n",
      "Confidence: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "target_review = testing_set[1][0]\n",
    "print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41589157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: pos\n",
      "Confidence: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "target_review = testing_set[2][0]\n",
    "print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d517d513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: pos\n",
      "Confidence: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "target_review = testing_set[3][0]\n",
    "print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "759ddb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: pos\n",
      "Confidence: 85.71428571428571 %\n"
     ]
    }
   ],
   "source": [
    "target_review = testing_set[4][0]\n",
    "print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86bf5e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: neg\n",
      "Confidence: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "target_review = testing_set[5][0]\n",
    "print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845a76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50a88218",
   "metadata": {},
   "source": [
    "# Investigating Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2d4ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dcb20fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (list(movie_reviews.words(fileId)), category) \n",
    "    for category in movie_reviews.categories() \n",
    "    for fileId in movie_reviews.fileids(category)\n",
    "]\n",
    "\n",
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab73cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive test set\n",
    "training_set = feature_sets[:1900]\n",
    "testing_set = feature_sets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1ae063ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative test set\n",
    "training_set = feature_sets[100:]\n",
    "testing_set = feature_sets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "45e26943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy: 79.00 %\n",
      "MNB_classifier accuracy: 80.00 %\n",
      "bernoulli_classifier accuracy: 80.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomm/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy: 73.00 %\n",
      "SGDClassifier_classifier accuracy: 73.00 %\n",
      "LinearSVC_classifier accuracy: 72.00 %\n",
      "NuSVC_classifier accuracy: 77.00 %\n",
      "voted_classifier accuracy: 81.00 %\n",
      "\n",
      "Looking at review 0\n",
      "Classification: pos\n",
      "Confidence: 71.42857142857143 %\n",
      "\n",
      "Looking at review 1\n",
      "Classification: neg\n",
      "Confidence: 100.0 %\n",
      "\n",
      "Looking at review 2\n",
      "Classification: neg\n",
      "Confidence: 100.0 %\n",
      "\n",
      "Looking at review 3\n",
      "Classification: neg\n",
      "Confidence: 57.14285714285714 %\n",
      "\n",
      "Looking at review 4\n",
      "Classification: neg\n",
      "Confidence: 57.14285714285714 %\n"
     ]
    }
   ],
   "source": [
    "original_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(f\"Original Naive Bayes Algo accuracy: {nltk.classify.accuracy(original_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(f\"MNB_classifier accuracy: {nltk.classify.accuracy(MNB_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(training_set)\n",
    "print(f\"bernoulli_classifier accuracy: {nltk.classify.accuracy(bernoulli_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(f\"LogisticRegression_classifier accuracy: {nltk.classify.accuracy(LogisticRegression_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(f\"SGDClassifier_classifier accuracy: {nltk.classify.accuracy(SGDClassifier_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(f\"LinearSVC_classifier accuracy: {nltk.classify.accuracy(LinearSVC_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(f\"NuSVC_classifier accuracy: {nltk.classify.accuracy(NuSVC_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "    original_classifier,\n",
    "    MNB_classifier,\n",
    "    bernoulli_classifier,\n",
    "    LogisticRegression_classifier,\n",
    "    SGDClassifier_classifier,\n",
    "    LinearSVC_classifier,\n",
    "    NuSVC_classifier,\n",
    ")\n",
    "print(f\"voted_classifier accuracy: {nltk.classify.accuracy(voted_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print()\n",
    "    print(f\"Looking at review {i}\")\n",
    "    target_review = testing_set[i][0]\n",
    "    print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "    print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb3d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab480c1c",
   "metadata": {},
   "source": [
    "# Better training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbe8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6072dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdfd2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e15849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_positives = open(\"tutorial_datasets/positive.txt\", \"r\").read()\n",
    "short_negatives = open(\"tutorial_datasets/negative.txt\", \"r\").read()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for review in short_positives.split(\"\\n\"):\n",
    "    documents.append( (review, \"pos\") )\n",
    "\n",
    "for review in short_negatives.split(\"\\n\"):\n",
    "    documents.append( (review, \"neg\") )\n",
    "\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = nltk.word_tokenize(short_positives)\n",
    "short_neg_words = nltk.word_tokenize(short_negatives)\n",
    "\n",
    "for word in short_pos_words:\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "for word in short_neg_words:\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "random.shuffle(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379f8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = feature_sets[:10_000]\n",
    "testing_set = feature_sets[10_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db39d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy: 71.90 %\n",
      "MNB_classifier accuracy: 72.05 %\n",
      "bernoulli_classifier accuracy: 72.51 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomm/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy: 71.75 %\n",
      "SGDClassifier_classifier accuracy: 71.15 %\n",
      "LinearSVC_classifier accuracy: 69.64 %\n",
      "NuSVC_classifier accuracy: 74.02 %\n",
      "voted_classifier accuracy: 74.17 %\n",
      "\n",
      "Looking at review 0\n",
      "Classification: neg\n",
      "Confidence: 85.71428571428571 %\n",
      "\n",
      "Looking at review 1\n",
      "Classification: pos\n",
      "Confidence: 100.0 %\n",
      "\n",
      "Looking at review 2\n",
      "Classification: pos\n",
      "Confidence: 71.42857142857143 %\n",
      "\n",
      "Looking at review 3\n",
      "Classification: pos\n",
      "Confidence: 57.14285714285714 %\n",
      "\n",
      "Looking at review 4\n",
      "Classification: neg\n",
      "Confidence: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "original_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(f\"Original Naive Bayes Algo accuracy: {nltk.classify.accuracy(original_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(f\"MNB_classifier accuracy: {nltk.classify.accuracy(MNB_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(training_set)\n",
    "print(f\"bernoulli_classifier accuracy: {nltk.classify.accuracy(bernoulli_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(f\"LogisticRegression_classifier accuracy: {nltk.classify.accuracy(LogisticRegression_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(f\"SGDClassifier_classifier accuracy: {nltk.classify.accuracy(SGDClassifier_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(f\"LinearSVC_classifier accuracy: {nltk.classify.accuracy(LinearSVC_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(f\"NuSVC_classifier accuracy: {nltk.classify.accuracy(NuSVC_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "    original_classifier,\n",
    "    MNB_classifier,\n",
    "    bernoulli_classifier,\n",
    "    LogisticRegression_classifier,\n",
    "    SGDClassifier_classifier,\n",
    "    LinearSVC_classifier,\n",
    "    NuSVC_classifier,\n",
    ")\n",
    "print(f\"voted_classifier accuracy: {nltk.classify.accuracy(voted_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print()\n",
    "    print(f\"Looking at review {i}\")\n",
    "    target_review = testing_set[i][0]\n",
    "    print(f\"Classification: {voted_classifier.classify(target_review)}\")\n",
    "    print(f\"Confidence: {voted_classifier.confidence(target_review) * 100.0} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29beaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the classifiers\n",
    "import pickle\n",
    "\n",
    "classifiers = {\n",
    "    \"bayes_classifier\": original_classifier,\n",
    "    \"mnb_classifier\": MNB_classifier,\n",
    "    \"bernoulli_classifier\": bernoulli_classifier,\n",
    "    \"logisticRegression_classifier\": LogisticRegression_classifier,\n",
    "    \"sgd_classifier\": SGDClassifier_classifier,\n",
    "    \"svc_classifier\": LinearSVC_classifier,\n",
    "    \"nuSvc_classifier\": NuSVC_classifier # This one is HUGE, probably best to leave\n",
    "}\n",
    "\n",
    "for classifier in classifiers:\n",
    "    save_classifier = open(f\"tutorial_pickles/{classifier}.pickle\", \"wb\")\n",
    "    pickle.dump(classifiers[classifier], save_classifier)\n",
    "    save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9937f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the other stuff\n",
    "\n",
    "save_all_words = open(\"tutorial_pickles/all_words.pickle\", \"wb\")\n",
    "pickle.dump(all_words, save_all_words)\n",
    "save_all_words.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d75a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86ed4ba",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ade91",
   "metadata": {},
   "source": [
    "## Generating everything and pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f316763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa51a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477fd45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0da43cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# J is adjective, R is adverb, and V is verb\n",
    "allowed_word_types = [\n",
    "    \"J\",\n",
    "#     \"R\",\n",
    "#     \"V\"\n",
    "]\n",
    "\n",
    "short_positives = open(\"tutorial_datasets/positive.txt\", \"r\").read()\n",
    "short_negatives = open(\"tutorial_datasets/negative.txt\", \"r\").read()\n",
    "\n",
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "for p in short_positives.split(\"\\n\"):\n",
    "    documents.append( (p, \"pos\") )\n",
    "    words = word_tokenize(p)\n",
    "    \n",
    "    # Get parts of speech\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "for p in short_negatives.split(\"\\n\"):\n",
    "    documents.append( (p, \"neg\") )\n",
    "    words = word_tokenize(p)\n",
    "    \n",
    "    # Get parts of speech\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "    \n",
    "save_documents = open(\"tutorial_pickles/documents.pickle\", \"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:5_000]\n",
    "\n",
    "save_word_features = open(\"tutorial_pickles/word_features.pickle\", \"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ac334",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(feature_sets)\n",
    "print(f\"Found {len(feature_sets)} feature sets\")\n",
    "\n",
    "save_feature_sets = open(\"tutorial_pickles/feature_sets.pickle\", \"wb\")\n",
    "pickle.dump(feature_sets, save_feature_sets)\n",
    "save_feature_sets.close()\n",
    "\n",
    "training_set = feature_sets[:10_000]\n",
    "testing_set = feature_sets[10_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(f\"Original Naive Bayes Algo accuracy: {nltk.classify.accuracy(original_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(f\"MNB_classifier accuracy: {nltk.classify.accuracy(MNB_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(training_set)\n",
    "print(f\"bernoulli_classifier accuracy: {nltk.classify.accuracy(bernoulli_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(f\"LogisticRegression_classifier accuracy: {nltk.classify.accuracy(LogisticRegression_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(f\"SGDClassifier_classifier accuracy: {nltk.classify.accuracy(SGDClassifier_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(f\"LinearSVC_classifier accuracy: {nltk.classify.accuracy(LinearSVC_classifier, testing_set) * 100.0:.2f} %\")\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "    original_classifier,\n",
    "    MNB_classifier,\n",
    "    bernoulli_classifier,\n",
    "    LogisticRegression_classifier,\n",
    "    SGDClassifier_classifier,\n",
    "    LinearSVC_classifier\n",
    ")\n",
    "print(f\"voted_classifier accuracy: {nltk.classify.accuracy(voted_classifier, testing_set) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468528d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the classifiers\n",
    "import pickle\n",
    "\n",
    "classifiers = {\n",
    "    \"bayes_classifier\": original_classifier,\n",
    "    \"mnb_classifier\": MNB_classifier,\n",
    "    \"bernoulli_classifier\": bernoulli_classifier,\n",
    "    \"logisticRegression_classifier\": LogisticRegression_classifier,\n",
    "    \"sgd_classifier\": SGDClassifier_classifier,\n",
    "    \"svc_classifier\": LinearSVC_classifier\n",
    "}\n",
    "\n",
    "for classifier in classifiers:\n",
    "    save_classifier = open(f\"tutorial_pickles/{classifier}.pickle\", \"wb\")\n",
    "    pickle.dump(classifiers[classifier], save_classifier)\n",
    "    save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f688826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    \n",
    "    return voted_classifier.classify(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf59f1",
   "metadata": {},
   "source": [
    "## Treating it as a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You would need to save this as something like \"sentiment_mod.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf359fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc96968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed21a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def read_pickle(pickle_name):\n",
    "    opener = open(f\"tutorial_pickles/{pickle_name}.pickle\", \"rb\")\n",
    "    result = pickle.load(opener, encoding=\"latin-1\")\n",
    "    opener.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e93457e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bayes_classifier.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e7338f9da57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bayes_classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mMNB_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnb_classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbernoulli_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bernoulli_classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLogisticRegression_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logisticRegression_classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSGDClassifier_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sgd_classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f9bbd9f68510>\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(pickle_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{pickle_name}.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bayes_classifier.pickle'"
     ]
    }
   ],
   "source": [
    "original_classifier = read_pickle(\"bayes_classifier\")\n",
    "MNB_classifier = read_pickle(\"mnb_classifier\")\n",
    "bernoulli_classifier = read_pickle(\"bernoulli_classifier\")\n",
    "LogisticRegression_classifier = read_pickle(\"logisticRegression_classifier\")\n",
    "SGDClassifier_classifier = read_pickle(\"sgd_classifier\")\n",
    "LinearSVC_classifier = read_pickle(\"svc_classifier\")\n",
    "\n",
    "documents = read_pickle(\"documents\")\n",
    "all_words = read_pickle(\"all_words\")\n",
    "word_features = read_pickle(\"word_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f03aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10662 feature sets\n"
     ]
    }
   ],
   "source": [
    "# Should probably pickle this but it gets huge so no thank you\n",
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(feature_sets)\n",
    "print(f\"Found {len(feature_sets)} feature sets\")\n",
    "\n",
    "training_set = feature_sets[:10_000]\n",
    "testing_set = feature_sets[10_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcd9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    \n",
    "    return voted_classifier.classify(feats), voted_classifier.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7251f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(\n",
    "    original_classifier,\n",
    "    MNB_classifier,\n",
    "    bernoulli_classifier,\n",
    "    LogisticRegression_classifier,\n",
    "    SGDClassifier_classifier,\n",
    "    LinearSVC_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c2360",
   "metadata": {},
   "source": [
    "## Bringing it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86386046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we actually saved it as sentiment_mod.py\n",
    "# import sentiment_mod as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d82aba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 1.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons... so yeah!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf89b62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 1.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ccd5a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"TSLA is looking pretty bullish right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b2186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ceb19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b0e2e6b",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae81eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a428d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5a34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, pretend we have actually imported this\n",
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    \n",
    "    return voted_classifier.classify(feats), voted_classifier.confidence(feats)\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def read_pickle(pickle_name):\n",
    "    opener = open(f\"{pickle_name}.pickle\", \"rb\")\n",
    "    result = pickle.load(opener, encoding=\"latin-1\")\n",
    "    opener.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9881b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3212f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10662 feature sets\n"
     ]
    }
   ],
   "source": [
    "original_classifier = read_pickle(\"tutorial_pickles/bayes_classifier\")\n",
    "MNB_classifier = read_pickle(\"tutorial_pickles/mnb_classifier\")\n",
    "bernoulli_classifier = read_pickle(\"tutorial_pickles/bernoulli_classifier\")\n",
    "LogisticRegression_classifier = read_pickle(\"tutorial_pickles/logisticRegression_classifier\")\n",
    "SGDClassifier_classifier = read_pickle(\"tutorial_pickles/sgd_classifier\")\n",
    "LinearSVC_classifier = read_pickle(\"tutorial_pickles/svc_classifier\")\n",
    "\n",
    "documents = read_pickle(\"tutorial_pickles/documents\")\n",
    "all_words = read_pickle(\"tutorial_pickles/all_words\")\n",
    "word_features = read_pickle(\"tutorial_pickles/word_features\")\n",
    "\n",
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "random.shuffle(feature_sets)\n",
    "print(f\"Found {len(feature_sets)} feature sets\")\n",
    "\n",
    "training_set = feature_sets[:10_000]\n",
    "testing_set = feature_sets[10_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af07990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class listener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        all_data = json.loads(data)\n",
    "\n",
    "        tweet = all_data[\"text\"]\n",
    "        \n",
    "        sentiment_value, confidence = sentiment(tweet)\n",
    "\n",
    "        print(tweet)\n",
    "        print(f\"Sentiment: {sentiment_value}\")\n",
    "        print(f\"Confidence: {confidence}\")\n",
    "        \n",
    "        if confidence * 100.0 >= 80.0:\n",
    "            output = open(\"tutorial_datasets/twitter-out.txt\", \"a\")\n",
    "            output.write(sentiment_value)\n",
    "            output.write(\"\\n\")\n",
    "            output.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a31826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(\n",
    "    original_classifier,\n",
    "    MNB_classifier,\n",
    "    bernoulli_classifier,\n",
    "    LogisticRegression_classifier,\n",
    "    SGDClassifier_classifier,\n",
    "    LinearSVC_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "393be409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "twitter_helper = helper.TwitterHelper()\n",
    "reddit_helper = helper.RedditHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1cd9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ailyn11ac: i'm selling my gfs jeep for bad bunny ticketslmk if you're interested... serious inquiries only https://t.co/GqPKwFDvC6\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "@nuggets Honestly i still truly believe nba is rigged. Its basketball entertainment. They just feature certain guys https://t.co/sdBYpHOmiq\n",
      "Sentiment: neg\n",
      "Confidence: 0.5\n",
      "@jeepsforlifee @davidmweissman Lololll also, you insisting the factual statement is that people get by without cars https://t.co/ozKAlJF1Vb\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "https://t.co/VjhyIWaxdH\n",
      "\n",
      "2021 Jeep Gladiator Pickup's New Texas Trail Model Is Exclusively for Texans\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "Dont ignore your mirrors\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "There are some people who make me want to choose violence sometimes .... especially the asshole who threw a drink a https://t.co/s6UnqaxZeZ\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "My little brother gets my Jeep stuck in the creek. It had to stay over night shit sucked. Our friends dad came to r https://t.co/2M7ljGtGHS\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "@PakFirst14 @GuardianZ_10 Meanwhile the one who crushed the policeman under his jeep got free, kashmala Tariq also https://t.co/v7P5IKS1yn\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "hays kamiss bumyahe tas ganto tugtugan sa jeep\n",
      "https://t.co/ZQ0rllOvZ7\n",
      "Sentiment: pos\n",
      "Confidence: 1.0\n",
      "@AllVolEveryday @Smoothnihilist @shannonrwatts Oh and I drive a jeep so not very fast.\n",
      "Sentiment: pos\n",
      "Confidence: 1.0\n",
      "If I got gifted a Jeep, I dont even know why Id do.\n",
      "Sentiment: pos\n",
      "Confidence: 0.5\n",
      "RT @oseazam: So... Anyone wanna buy me a jeep wrangler? \n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "@justaguy1766 @ImSpeaking13 Some old guy MAGA quote tweeted Kate yesterday and they went crazy. His profile was the https://t.co/r7dTYThqDi\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n",
      "White Jeep\n",
      "Sentiment: neg\n",
      "Confidence: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e61effd8411c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtwitterStream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtwitterStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"JEEP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filter_level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delimited'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, is_async)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[0;34m(self, resp)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mstripped_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mline\u001b[0m \u001b[0;31m# line is sometimes None so we need to check here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstripped_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/nlp-exploration-V-XCybpD-py3.8/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m                 if (\n\u001b[1;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Read the next chunk size from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collect these from an external file (not on GitHub)\n",
    "#consumer key, consumer secret, access token, access secret.\n",
    "ckey = twitter_helper.api_key\n",
    "csecret = twitter_helper.secret_key\n",
    "atoken = twitter_helper.access_token\n",
    "asecret = twitter_helper.access_secret\n",
    "\n",
    "        \n",
    "auth = OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track=[\"JEEP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5863db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fa1d2da",
   "metadata": {},
   "source": [
    "# Graphing Live Twitter sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee8b4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bca8338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "style.use(\"ggplot\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "def animate(i):\n",
    "    pullData = open(\"tutorial_datasets/twitter-out.txt\", \"r\").read()\n",
    "    lines = pullData.split('\\n')\n",
    "    \n",
    "    xar = []\n",
    "    yar = []\n",
    "    \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for l in lines[-200:]:\n",
    "        x += 1\n",
    "        if \"pos\" in l:\n",
    "            y += 1\n",
    "        elif \"neg\" in l:\n",
    "            y -= 1\n",
    "        \n",
    "        xar.append(x)\n",
    "        yar.append(y)\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.plot(xar,yar)\n",
    "    \n",
    "ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e3091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ff379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df15b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcfa15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb4fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7760bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12907cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba88ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7933a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eba041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e883d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7510b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be22f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135dc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83a646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
